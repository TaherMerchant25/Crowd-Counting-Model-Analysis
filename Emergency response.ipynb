{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "25WN2ng4oMWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision.ops import nms\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "3VtIN6tpoSoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logging for emergency response\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('emergency_response.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EmergencyResponseConfig:\n",
        "    \"\"\"Configuration class for disaster response crowd counting\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Device configuration\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Model parameters (matching your trained model)\n",
        "        self.IMG_HEIGHT = 256\n",
        "        self.IMG_WIDTH = 256\n",
        "        self.STRIDE = 8\n",
        "\n",
        "        # Detection thresholds optimized for emergency scenarios\n",
        "        self.CONFIDENCE_THRESHOLD = 0.2\n",
        "        self.DBSCAN_EPS = 15\n",
        "        self.DBSCAN_MIN_SAMPLES = 1\n",
        "\n",
        "        # Emergency response parameters\n",
        "        self.HIGH_DENSITY_THRESHOLD = 50  # People per area unit\n",
        "        self.CRITICAL_DENSITY_THRESHOLD = 100\n",
        "        self.EVACUATION_ZONE_SIZE = 100  # pixels\n",
        "\n",
        "        # File paths\n",
        "        self.MODEL_PATH = '/content/drive/MyDrive/attention_denisity_based.pth'\n",
        "        self.OUTPUT_DIR = '/content/drive/MyDrive/crowd_test_data'\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Alert colors for different crowd density levels\n",
        "        self.COLORS = {\n",
        "            'safe': (0, 255, 0),      # Green\n",
        "            'moderate': (0, 255, 255), # Yellow\n",
        "            'high': (0, 165, 255),     # Orange\n",
        "            'critical': (0, 0, 255),   # Red\n",
        "            'evacuation': (255, 0, 255) # Magenta\n",
        "        }\n"
      ],
      "metadata": {
        "id": "8qMxQk-CocH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Used**"
      ],
      "metadata": {
        "id": "OCaU_dyCoe_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleBlock(nn.Module):\n",
        "    \"\"\"Multi-scale feature extraction block\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, 5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels, 7, padding=3)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "        self.fuse = nn.Conv2d(in_channels * 3, in_channels, 1)\n",
        "        self.bn_fuse = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.relu(self.bn1(self.conv1(x)))\n",
        "        x2 = self.relu(self.bn2(self.conv2(x)))\n",
        "        x3 = self.relu(self.bn3(self.conv3(x)))\n",
        "        return self.relu(self.bn_fuse(self.fuse(torch.cat([x1, x2, x3], dim=1))))\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Attention mechanism for better feature focus\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 8, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // 8, in_channels, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.avg_pool(x)\n",
        "        w = self.fc(w)\n",
        "        return x * w\n",
        "\n",
        "class APGCCPointNet(nn.Module):\n",
        "    \"\"\"APGCC Point-based crowd counting network\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg16_features = models.vgg16(pretrained=True).features\n",
        "        self.encoder = nn.Sequential(*list(vgg16_features)[:17])\n",
        "        ENCODER_OUT_CHANNELS = 256\n",
        "\n",
        "        self.multiscale_attn_block = nn.Sequential(\n",
        "            MultiScaleBlock(ENCODER_OUT_CHANNELS),\n",
        "            AttentionBlock(ENCODER_OUT_CHANNELS)\n",
        "        )\n",
        "\n",
        "        self.confidence_head = nn.Conv2d(ENCODER_OUT_CHANNELS, 1, 1, bias=True)\n",
        "        self.offset_head = nn.Conv2d(ENCODER_OUT_CHANNELS, 2, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature_map = self.encoder(x)\n",
        "        processed_feature_map = self.multiscale_attn_block(feature_map)\n",
        "        confidence_output = self.confidence_head(processed_feature_map)\n",
        "        offset_output = self.offset_head(processed_feature_map)\n",
        "        return confidence_output, offset_output"
      ],
      "metadata": {
        "id": "dtgLAsK8omUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmergencyResponseAnalyzer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.emergency_log = []\n",
        "\n",
        "        # Initialize preprocessing pipeline\n",
        "        self.inference_transforms = A.Compose([\n",
        "            A.Resize(config.IMG_HEIGHT, config.IMG_WIDTH, interpolation=cv2.INTER_AREA),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "        logger.info(f\"Emergency Response Analyzer initialized on device: {config.device}\")"
      ],
      "metadata": {
        "id": "JOzkHNiho1ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Model**"
      ],
      "metadata": {
        "id": "YsieH2Lyo2ob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79mCjYIgoGdR"
      },
      "outputs": [],
      "source": [
        "    def load_model(self):\n",
        "        \"\"\"Load the trained crowd counting model\"\"\"\n",
        "        try:\n",
        "            self.model = APGCCPointNet().to(self.config.device)\n",
        "            if os.path.exists(self.config.MODEL_PATH):\n",
        "                self.model.load_state_dict(torch.load(self.config.MODEL_PATH, map_location=self.config.device))\n",
        "                self.model.eval()\n",
        "                logger.info(f\"Model loaded successfully from {self.config.MODEL_PATH}\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.error(f\"Model file not found: {self.config.MODEL_PATH}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_crowd_density(self, points, frame_shape):\n",
        "        \"\"\"Analyze crowd density and identify emergency zones\"\"\"\n",
        "        if len(points) == 0:\n",
        "            return [], \"safe\", 0\n",
        "\n",
        "        h, w = frame_shape[:2]\n",
        "\n",
        "        # Create density map\n",
        "        density_zones = []\n",
        "        grid_size = self.config.EVACUATION_ZONE_SIZE\n",
        "\n",
        "        for y in range(0, h, grid_size):\n",
        "            for x in range(0, w, grid_size):\n",
        "                zone_points = []\n",
        "                for point in points:\n",
        "                    px, py = point\n",
        "                    if x <= px < x + grid_size and y <= py < y + grid_size:\n",
        "                        zone_points.append(point)\n",
        "\n",
        "                if len(zone_points) > 0:\n",
        "                    density = len(zone_points)\n",
        "                    zone_info = {\n",
        "                        'bbox': (x, y, min(x + grid_size, w), min(y + grid_size, h)),\n",
        "                        'count': density,\n",
        "                        'points': zone_points,\n",
        "                        'center': (x + grid_size//2, y + grid_size//2)\n",
        "                    }\n",
        "\n",
        "                    # Classify density level\n",
        "                    if density >= self.config.CRITICAL_DENSITY_THRESHOLD:\n",
        "                        zone_info['level'] = 'critical'\n",
        "                    elif density >= self.config.HIGH_DENSITY_THRESHOLD:\n",
        "                        zone_info['level'] = 'high'\n",
        "                    else:\n",
        "                        zone_info['level'] = 'moderate'\n",
        "\n",
        "                    density_zones.append(zone_info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Determining Emergency Level and Visualization**"
      ],
      "metadata": {
        "id": "OcUNF7sEpMLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        # Determine overall emergency level\n",
        "        total_count = len(points)\n",
        "        max_zone_density = max([zone['count'] for zone in density_zones]) if density_zones else 0\n",
        "\n",
        "        if max_zone_density >= self.config.CRITICAL_DENSITY_THRESHOLD:\n",
        "            emergency_level = 'critical'\n",
        "        elif max_zone_density >= self.config.HIGH_DENSITY_THRESHOLD:\n",
        "            emergency_level = 'high'\n",
        "        elif total_count > 20:\n",
        "            emergency_level = 'moderate'\n",
        "        else:\n",
        "            emergency_level = 'safe'\n",
        "\n",
        "        return density_zones, emergency_level, total_count\n",
        "\n",
        "    def log_emergency_event(self, frame_idx, total_count, emergency_level, density_zones):\n",
        "        \"\"\"Log emergency events for response coordination\"\"\"\n",
        "        event = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'frame': frame_idx,\n",
        "            'total_count': total_count,\n",
        "            'emergency_level': emergency_level,\n",
        "            'high_density_zones': len([z for z in density_zones if z['level'] in ['high', 'critical']]),\n",
        "            'critical_zones': len([z for z in density_zones if z['level'] == 'critical'])\n",
        "        }\n",
        "\n",
        "        self.emergency_log.append(event)\n",
        "\n",
        "        if emergency_level in ['high', 'critical']:\n",
        "            logger.warning(f\"EMERGENCY ALERT - Frame {frame_idx}: {emergency_level.upper()} density detected - {total_count} people\")\n",
        "\n",
        "    def draw_emergency_visualization(self, frame, points, density_zones, emergency_level, total_count, frame_idx):\n",
        "        \"\"\"Draw comprehensive emergency response visualization\"\"\"\n",
        "        overlay = frame.copy()\n",
        "\n",
        "        # Draw density zones\n",
        "        for zone in density_zones:\n",
        "            x1, y1, x2, y2 = zone['bbox']\n",
        "            color = self.config.COLORS[zone['level']]\n",
        "\n",
        "            # Draw zone rectangle\n",
        "            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Draw zone info\n",
        "            zone_text = f\"{zone['count']} people\"\n",
        "            cv2.putText(overlay, zone_text, (x1, y1-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "        # Draw individual detected people\n",
        "        for point in points:\n",
        "            x, y = int(point[0]), int(point[1])\n",
        "            cv2.circle(overlay, (x, y), 3, (255, 255, 255), -1)\n",
        "            cv2.circle(overlay, (x, y), 5, (0, 0, 0), 1)\n",
        "\n",
        "        # Emergency status panel\n",
        "        panel_height = 120\n",
        "        panel_color = self.config.COLORS[emergency_level]\n",
        "        cv2.rectangle(overlay, (10, 10), (400, panel_height), (0, 0, 0), -1)\n",
        "        cv2.rectangle(overlay, (10, 10), (400, panel_height), panel_color, 3)\n",
        "\n",
        "        # Status text\n",
        "        status_texts = [\n",
        "            f\"EMERGENCY STATUS: {emergency_level.upper()}\",\n",
        "            f\"Total People Detected: {total_count}\",\n",
        "            f\"High-Density Zones: {len([z for z in density_zones if z['level'] in ['high', 'critical']])}\",\n",
        "            f\"Frame: {frame_idx} | Time: {datetime.now().strftime('%H:%M:%S')}\"\n",
        "        ]\n",
        "\n",
        "        for i, text in enumerate(status_texts):\n",
        "            y_pos = 30 + i * 20\n",
        "            cv2.putText(overlay, text, (20, y_pos),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
        "\n",
        "        # Emergency instructions\n",
        "        if emergency_level in ['high', 'critical']:\n",
        "            instruction_y = frame.shape[0] - 60\n",
        "            instructions = [\n",
        "                \"EMERGENCY RESPONSE REQUIRED\",\n",
        "                \"Deploy rescue teams to high-density zones\",\n",
        "                \"Coordinate evacuation routes\"\n",
        "            ]\n",
        "\n",
        "            for i, instruction in enumerate(instructions):\n",
        "                cv2.putText(overlay, instruction, (20, instruction_y + i * 20),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "        return overlay\n",
        "\n",
        "    def process_video(self, video_path = \"/content/drive/MyDrive/crowd_test_data/1.mp4\", output_path= \"/content/drive/MyDrive/crowd_test_data\"):\n",
        "        \"\"\"Process video for emergency response crowd analysis\"\"\"\n",
        "        if not self.model:\n",
        "            logger.error(\"Model not loaded. Call load_model() first.\")\n",
        "            return False\n",
        "\n",
        "        # Setup video capture\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            logger.error(f\"Could not open video: {video_path}\")\n",
        "            return False\n",
        "\n",
        "        # Get video properties\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        logger.info(f\"Processing video: {video_path}\")\n",
        "        logger.info(f\"Video properties: {frame_width}x{frame_height}, {fps} FPS, {total_frames} frames\")\n",
        "\n",
        "        # Setup output video writer\n",
        "        if output_path is None:\n",
        "            output_path = os.path.join(self.config.OUTPUT_DIR, f'emergency_analysis_{int(time.time())}.mp4')\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        if not out.isOpened():\n",
        "            logger.error(\"Could not create output video writer\")\n",
        "            return False\n",
        "\n",
        "        frame_idx = 0\n",
        "        emergency_alerts = 0\n",
        "\n",
        "        logger.info(\"Starting emergency response analysis...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Preprocess frame\n",
        "                original_frame = frame.copy()\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                transformed = self.inference_transforms(image=frame_rgb)\n",
        "                image_tensor = transformed['image'].unsqueeze(0).to(self.config.device)\n",
        "\n",
        "                # Model inference\n",
        "                y_pred_conf_logits, y_pred_offset = self.model(image_tensor)\n",
        "                pred_conf_map = torch.sigmoid(y_pred_conf_logits).squeeze(0).squeeze(0).cpu().numpy()\n",
        "                pred_offset_map = y_pred_offset.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "                # Extract predicted points\n",
        "                predicted_points_raw = []\n",
        "                feature_map_height, feature_map_width = pred_conf_map.shape\n",
        "\n",
        "                for fy in range(feature_map_height):\n",
        "                    for fx in range(feature_map_width):\n",
        "                        conf = pred_conf_map[fy, fx]\n",
        "                        if conf >= self.config.CONFIDENCE_THRESHOLD:\n",
        "                            pred_x = (fx * self.config.STRIDE) + (self.config.STRIDE / 2) + (pred_offset_map[fy, fx, 0] * self.config.STRIDE)\n",
        "                            pred_y = (fy * self.config.STRIDE) + (self.config.STRIDE / 2) + (pred_offset_map[fy, fx, 1] * self.config.STRIDE)\n",
        "                            predicted_points_raw.append([pred_x, pred_y])\n",
        "\n",
        "                # Apply DBSCAN clustering\n",
        "                clustered_points = []\n",
        "                if predicted_points_raw:\n",
        "                    predicted_points_np = np.array(predicted_points_raw, dtype=np.float32)\n",
        "                    clustering = DBSCAN(eps=self.config.DBSCAN_EPS, min_samples=self.config.DBSCAN_MIN_SAMPLES).fit(predicted_points_np)\n",
        "                    labels = clustering.labels_\n",
        "\n",
        "                    for k in set(labels):\n",
        "                        if k == -1:\n",
        "                            continue\n",
        "                        class_members = predicted_points_np[labels == k]\n",
        "                        cluster_center = np.mean(class_members, axis=0)\n",
        "                        clustered_points.append(cluster_center)\n",
        "\n",
        "                # Scale points to original frame dimensions\n",
        "                scale_x = frame_width / self.config.IMG_WIDTH\n",
        "                scale_y = frame_height / self.config.IMG_HEIGHT\n",
        "                scaled_points = [[p[0] * scale_x, p[1] * scale_y] for p in clustered_points]\n",
        "\n",
        "                # Analyze crowd density for emergency response\n",
        "                density_zones, emergency_level, total_count = self.analyze_crowd_density(scaled_points, frame.shape)\n",
        "\n",
        "                # Log emergency events\n",
        "                self.log_emergency_event(frame_idx, total_count, emergency_level, density_zones)\n",
        "\n",
        "                if emergency_level in ['high', 'critical']:\n",
        "                    emergency_alerts += 1\n",
        "\n",
        "                # Create visualization\n",
        "                visualized_frame = self.draw_emergency_visualization(\n",
        "                    original_frame, scaled_points, density_zones, emergency_level, total_count, frame_idx\n",
        "                )\n",
        "\n",
        "                # Write frame to output video\n",
        "                out.write(visualized_frame)\n",
        "\n",
        "                frame_idx += 1\n",
        "\n",
        "                # Progress logging\n",
        "                if frame_idx % 30 == 0:  # Log every 30 frames\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    logger.info(f\"Progress: {progress:.1f}% - Frame {frame_idx}/{total_frames} - Emergency alerts: {emergency_alerts}\")\n",
        "\n",
        "        # Cleanup\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Save emergency log\n",
        "        log_path = os.path.join(self.config.OUTPUT_DIR, f'emergency_log_{int(time.time())}.json')\n",
        "        with open(log_path, 'w') as f:\n",
        "            json.dump(self.emergency_log, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Analysis complete!\")\n",
        "        logger.info(f\"Output video: {output_path}\")\n",
        "        logger.info(f\"Emergency log: {log_path}\")\n",
        "        logger.info(f\"Total emergency alerts: {emergency_alerts}\")\n",
        "        logger.info(f\"Processed {frame_idx} frames\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def process_live_feed(self, source=0):\n",
        "        \"\"\"Process live camera feed for real-time emergency monitoring\"\"\"\n",
        "        if not self.model:\n",
        "            logger.error(\"Model not loaded. Call load_model() first.\")\n",
        "            return False\n",
        "\n",
        "        cap = cv2.VideoCapture(source)\n",
        "        if not cap.isOpened():\n",
        "            logger.error(f\"Could not open camera source: {source}\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"Starting live emergency monitoring. Press 'q' to quit.\")\n",
        "\n",
        "        frame_idx = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Process frame (similar to video processing)\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                transformed = self.inference_transforms(image=frame_rgb)\n",
        "                image_tensor = transformed['image'].unsqueeze(0).to(self.config.device)\n",
        "\n",
        "                y_pred_conf_logits, y_pred_offset = self.model(image_tensor)\n",
        "                pred_conf_map = torch.sigmoid(y_pred_conf_logits).squeeze(0).squeeze(0).cpu().numpy()\n",
        "                pred_offset_map = y_pred_offset.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "                # Extract and cluster points\n",
        "                predicted_points_raw = []\n",
        "                feature_map_height, feature_map_width = pred_conf_map.shape\n",
        "\n",
        "                for fy in range(feature_map_height):\n",
        "                    for fx in range(feature_map_width):\n",
        "                        conf = pred_conf_map[fy, fx]\n",
        "                        if conf >= self.config.CONFIDENCE_THRESHOLD:\n",
        "                            pred_x = (fx * self.config.STRIDE) + (self.config.STRIDE / 2) + (pred_offset_map[fy, fx, 0] * self.config.STRIDE)\n",
        "                            pred_y = (fy * self.config.STRIDE) + (self.config.STRIDE / 2) + (pred_offset_map[fy, fx, 1] * self.config.STRIDE)\n",
        "                            predicted_points_raw.append([pred_x, pred_y])\n",
        "\n",
        "                clustered_points = []\n",
        "                if predicted_points_raw:\n",
        "                    predicted_points_np = np.array(predicted_points_raw, dtype=np.float32)\n",
        "                    clustering = DBSCAN(eps=self.config.DBSCAN_EPS, min_samples=self.config.DBSCAN_MIN_SAMPLES).fit(predicted_points_np)\n",
        "                    labels = clustering.labels_\n",
        "\n",
        "                    for k in set(labels):\n",
        "                        if k == -1:\n",
        "                            continue\n",
        "                        class_members = predicted_points_np[labels == k]\n",
        "                        cluster_center = np.mean(class_members, axis=0)\n",
        "                        clustered_points.append(cluster_center)\n",
        "\n",
        "                # Scale points\n",
        "                scale_x = frame.shape[1] / self.config.IMG_WIDTH\n",
        "                scale_y = frame.shape[0] / self.config.IMG_HEIGHT\n",
        "                scaled_points = [[p[0] * scale_x, p[1] * scale_y] for p in clustered_points]\n",
        "\n",
        "                # Analyze density\n",
        "                density_zones, emergency_level, total_count = self.analyze_crowd_density(scaled_points, frame.shape)\n",
        "\n",
        "                # Create visualization\n",
        "                visualized_frame = self.draw_emergency_visualization(\n",
        "                    frame, scaled_points, density_zones, emergency_level, total_count, frame_idx\n",
        "                )\n",
        "\n",
        "                # Display frame\n",
        "                cv2.imshow('Emergency Response - Live Monitoring', visualized_frame)\n",
        "\n",
        "                frame_idx += 1\n",
        "\n",
        "                # Check for quit\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        logger.info(\"Live monitoring stopped.\")\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for emergency response crowd analysis\"\"\"\n",
        "    parser = argparse.ArgumentParser(description='Emergency Response Crowd Density Analyzer')\n",
        "    parser.add_argument('--video', type=str, help='Path to input video file')\n",
        "    parser.add_argument('--live', action='store_true', help='Use live camera feed')\n",
        "    parser.add_argument('--output', type=str, help='Output video path')\n",
        "    parser.add_argument('--model', type=str, default='best_apgcc_model.pth', help='Path to model weights')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = EmergencyResponseConfig()\n",
        "    if args.model:\n",
        "        config.MODEL_PATH = args.model\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = EmergencyResponseAnalyzer(config)\n",
        "\n",
        "    # Load model\n",
        "    if not analyzer.load_model():\n",
        "        logger.error(\"Failed to load model. Exiting.\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"EMERGENCY RESPONSE CROWD DENSITY ANALYZER\")\n",
        "    logger.info(\"Optimized for disaster and evacuation scenarios\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        if args.live:\n",
        "            # Live camera monitoring\n",
        "            analyzer.process_live_feed()\n",
        "        elif args.video:\n",
        "            # Video file processing\n",
        "            analyzer.process_video(args.video, args.output)\n",
        "        else:\n",
        "            logger.error(\"Please specify either --video or --live option\")\n",
        "            parser.print_help()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"Process interrupted by user\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {e}\")\n",
        "\n",
        "    logger.info(\"Emergency response analysis completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "oLdhecsQpJuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}